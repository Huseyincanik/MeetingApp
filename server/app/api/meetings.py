from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks, Body
from sqlalchemy.orm import Session
from datetime import datetime
from typing import List, Optional
from pydantic import BaseModel
from ..database import get_db, SessionLocal
from ..models import User, Meeting
from ..schemas import MeetingCreate, MeetingResponse, MeetingUpdate
from ..api.auth import get_current_user
from ..services.meeting_service import MeetingService
from ..services.audio_service import AudioService

router = APIRouter()


@router.post("/start", response_model=MeetingResponse, status_code=status.HTTP_201_CREATED)
async def start_meeting(
    meeting_data: MeetingCreate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ToplantÄ± baÅŸlat"""
    # Aktif toplantÄ± kontrolÃ¼
    active_meeting = db.query(Meeting).filter(
        Meeting.user_id == current_user.id,
        Meeting.status.in_(["recording", "paused"])
    ).first()
    
    if active_meeting:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Zaten aktif bir toplantÄ±nÄ±z var"
        )
    
    # Yeni toplantÄ± oluÅŸtur
    new_meeting = Meeting(
        user_id=current_user.id,
        title=meeting_data.title or f"ToplantÄ± {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        whisper_model=meeting_data.whisper_model,
        language=meeting_data.language,
        status="recording",
        start_time=datetime.utcnow(),
        use_pyannote=str(meeting_data.use_pyannote).lower() if meeting_data.use_pyannote is not None else None,
        diarization_profile=meeting_data.diarization_profile,
        min_speakers=meeting_data.min_speakers,
        max_speakers=meeting_data.max_speakers
    )
    
    db.add(new_meeting)
    db.commit()
    db.refresh(new_meeting)
    
    return new_meeting


@router.post("/{meeting_id}/pause", response_model=MeetingResponse)
async def pause_meeting(
    meeting_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ToplantÄ±yÄ± duraklat"""
    meeting = db.query(Meeting).filter(
        Meeting.id == meeting_id,
        Meeting.user_id == current_user.id
    ).first()
    
    if not meeting:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="ToplantÄ± bulunamadÄ±"
        )
    
    if meeting.status != "recording":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Sadece kayÄ±t durumundaki toplantÄ±lar duraklatÄ±labilir"
        )
    
    meeting.status = "paused"
    meeting.pause_time = datetime.utcnow()
    db.commit()
    db.refresh(meeting)
    
    return meeting


@router.post("/{meeting_id}/resume", response_model=MeetingResponse)
async def resume_meeting(
    meeting_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ToplantÄ±yÄ± devam ettir"""
    meeting = db.query(Meeting).filter(
        Meeting.id == meeting_id,
        Meeting.user_id == current_user.id
    ).first()
    
    if not meeting:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="ToplantÄ± bulunamadÄ±"
        )
    
    if meeting.status != "paused":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Sadece duraklatÄ±lmÄ±ÅŸ toplantÄ±lar devam ettirilebilir"
        )
    
    meeting.status = "recording"
    meeting.pause_time = None
    meeting.silence_duration = 0
    db.commit()
    db.refresh(meeting)
    
    return meeting


@router.post("/{meeting_id}/end", response_model=MeetingResponse)
async def end_meeting(
    meeting_id: int,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ToplantÄ±yÄ± bitir"""
    meeting = db.query(Meeting).filter(
        Meeting.id == meeting_id,
        Meeting.user_id == current_user.id
    ).first()
    
    if not meeting:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="ToplantÄ± bulunamadÄ±"
        )
    
    if meeting.status == "completed":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="ToplantÄ± zaten tamamlanmÄ±ÅŸ"
        )
    
    meeting.status = "processing"
    meeting.end_time = datetime.utcnow()
    db.commit()
    
    # Background task ile transkript ve Ã¶zet oluÅŸtur
    from ..services.whisper_service import WhisperService
    from ..services.openai_service import OpenAIService
    import asyncio
    
    def process_meeting():
        if meeting.audio_file_path:
            # Audio preprocessing servisi
            from ..services.audio_preprocessing_service import AudioPreprocessingService
            from ..services.whisper_service import WhisperService
            from ..services.speaker_diarization_service import SpeakerDiarizationService
            
            preprocessing_service = AudioPreprocessingService()
            whisper_service = WhisperService()
            diarization_service = SpeakerDiarizationService()
            
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                print(f"ğŸ”Š Audio preprocessing baÅŸlatÄ±lÄ±yor: {meeting.audio_file_path}")
                
                # 1. Audio preprocessing (gÃ¼rÃ¼ltÃ¼ engelleme, normalizasyon)
                # SpeechRecognition, ElevenLabs ve AssemblyAI iÃ§in preprocessing'i atlayabiliriz (API'ler kendi iÅŸlemlerini yapÄ±yor)
                if meeting.whisper_model == "speechrecognition":
                    print("âš ï¸  SpeechRecognition iÃ§in preprocessing atlanÄ±yor (Google API kendi iÅŸlemlerini yapÄ±yor)")
                    processed_audio_path = meeting.audio_file_path
                elif meeting.whisper_model == "elevenlabs":
                    print("âš ï¸  ElevenLabs iÃ§in preprocessing atlanÄ±yor (ElevenLabs API kendi iÅŸlemlerini yapÄ±yor)")
                    processed_audio_path = meeting.audio_file_path
                elif meeting.whisper_model == "assemblyai":
                    print("âš ï¸  AssemblyAI iÃ§in preprocessing atlanÄ±yor (AssemblyAI API kendi iÅŸlemlerini yapÄ±yor)")
                    processed_audio_path = meeting.audio_file_path
                else:
                    processed_audio_path = preprocessing_service.preprocess_audio(meeting.audio_file_path)
                    print(f"âœ… Audio preprocessing tamamlandÄ±: {processed_audio_path}")
                
                # 2. Speaker diarization (konuÅŸmacÄ± ayÄ±rt etme) - Pyannote seÃ§eneÄŸi
                speaker_segments = []
                use_pyannote_diarization = (
                    meeting.use_pyannote == "true" or 
                    meeting.whisper_model == "pyannote"
                )
                
                # Pyannote diarization kullanÄ±lÄ±yorsa
                if use_pyannote_diarization or meeting.whisper_model == "pyannote":
                    print("ğŸ¤ Pyannote Diarization baÅŸlatÄ±lÄ±yor...")
                    try:
                        from ..services.pyannote_diarization_service import PyannoteDiarizationService
                        pyannote_service = PyannoteDiarizationService()
                        
                        # Pyannote ile transkript ve diarization
                        transcripts = pyannote_service.process_with_speakers(
                            processed_audio_path,
                            min_speakers=meeting.min_speakers,
                            max_speakers=meeting.max_speakers,
                            profile=meeting.diarization_profile or "auto"
                        )
                        
                        print(f"âœ… {len(transcripts)} transkript segmenti oluÅŸturuldu (Pyannote)")
                        
                        # VeritabanÄ±na kaydet
                        db_session = SessionLocal()
                        try:
                            from ..models import Transcript
                            for i, segment in enumerate(transcripts):
                                transcript = Transcript(
                                    meeting_id=meeting.id,
                                    segment_number=i + 1,
                                    text=segment["text"],
                                    start_time=segment["start"],
                                    end_time=segment["end"],
                                    speaker_id=segment.get("speaker_id"),
                                    speaker_label=segment.get("speaker_label")
                                )
                                db_session.add(transcript)
                            
                            updated_meeting = db_session.query(Meeting).filter(Meeting.id == meeting.id).first()
                            if updated_meeting:
                                updated_meeting.status = "completed"
                            db_session.commit()
                            print("âœ… Transkriptler veritabanÄ±na kaydedildi")
                        finally:
                            db_session.close()
                        
                        return  # Pyannote iÅŸlemi tamamlandÄ±, Ã§Ä±k
                        
                    except Exception as e:
                        print(f"âŒ Pyannote diarization hatasÄ±: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                else:
                    # Normal speaker diarization (eski yÃ¶ntem)
                    try:
                        print("ğŸ¤ Speaker diarization baÅŸlatÄ±lÄ±yor...")
                        speaker_segments = diarization_service.diarize(processed_audio_path)
                        if speaker_segments:
                            print(f"âœ… {len(speaker_segments)} speaker segmenti bulundu")
                        else:
                            print("âš ï¸  Speaker diarization sonuÃ§ vermedi (devam ediliyor)")
                    except Exception as e:
                        print(f"âš ï¸  Speaker diarization hatasÄ±: {e} (devam ediliyor)")
                
                # 3. Transkript oluÅŸtur (model tipine gÃ¶re)
                print(f"ğŸ“ Transkript oluÅŸturuluyor (Model: {meeting.whisper_model})...")
                
                # Model tipine gÃ¶re doÄŸru servisi kullan
                if meeting.whisper_model == "elevenlabs":
                    # ElevenLabs servisi kullan - KiÅŸi ayrÄ±mÄ± destekli
                    from ..services.elevenlabs_service import ElevenLabsService
                    elevenlabs_service = ElevenLabsService()
                    # ElevenLabs iÃ§in speaker diarization her zaman aktif
                    transcripts = loop.run_until_complete(
                        elevenlabs_service.transcribe_audio(
                            processed_audio_path,
                            model_name="elevenlabs",
                            language=meeting.language,
                            enable_speaker_diarization=True,  # ElevenLabs iÃ§in her zaman aktif
                            speaker_segments=speaker_segments  # Harici diarization sonuÃ§larÄ± ile birleÅŸtir
                        )
                    )
                elif meeting.whisper_model == "assemblyai":
                    # AssemblyAI servisi kullan - KiÅŸi ayrÄ±mÄ± destekli
                    from ..services.assemblyai_service import AssemblyAIService
                    assemblyai_service = AssemblyAIService()
                    # AssemblyAI iÃ§in speaker diarization aktif
                    transcripts = loop.run_until_complete(
                        assemblyai_service.transcribe_audio(
                            processed_audio_path,
                            model_name="assemblyai",
                            language=meeting.language,
                            enable_speaker_diarization=True,  # AssemblyAI iÃ§in aktif
                            speaker_segments=speaker_segments  # Harici diarization sonuÃ§larÄ± ile birleÅŸtir
                        )
                    )
                elif meeting.whisper_model == "speechrecognition":
                    from ..services.speechrecognition_service import SpeechRecognitionService
                    sr_service = SpeechRecognitionService()
                    transcripts = loop.run_until_complete(
                        sr_service.transcribe_audio(
                            processed_audio_path,
                            model_name="google",
                            language=meeting.language,
                            enable_speaker_diarization=len(speaker_segments) > 0,
                            speaker_segments=speaker_segments
                        )
                    )
                else:
                    # Whisper modeli kullan
                    transcripts = loop.run_until_complete(
                        whisper_service.transcribe_audio(
                            processed_audio_path,
                            meeting.whisper_model,
                            meeting.language,
                            enable_speaker_diarization=len(speaker_segments) > 0,
                            speaker_segments=speaker_segments
                        )
                    )
                print(f"âœ… {len(transcripts)} transkript segmenti oluÅŸturuldu")
                
                # 4. VeritabanÄ±na kaydet
                db_session = SessionLocal()
                try:
                    from ..models import Transcript
                    for i, segment in enumerate(transcripts):
                        print(f"ğŸ’¾ Transkript kaydediliyor - Segment {i+1}: '{segment['text'][:100]}...' ({len(segment['text'])} karakter)")
                        transcript = Transcript(
                            meeting_id=meeting.id,
                            segment_number=i + 1,
                            text=segment["text"],
                            start_time=segment["start"],
                            end_time=segment["end"],
                            speaker_id=segment.get("speaker_id"),
                            speaker_label=segment.get("speaker_label")
                        )
                        db_session.add(transcript)
                    
                    # ToplantÄ±yÄ± tamamla
                    updated_meeting = db_session.query(Meeting).filter(Meeting.id == meeting.id).first()
                    if updated_meeting:
                        updated_meeting.status = "completed"
                    db_session.commit()
                    print("âœ… Transkriptler veritabanÄ±na kaydedildi")
                finally:
                    db_session.close()
                
                # Temizlik: Ä°ÅŸlenmiÅŸ audio dosyasÄ±nÄ± sil (opsiyonel)
                # if processed_audio_path != meeting.audio_file_path:
                #     try:
                #         os.remove(processed_audio_path)
                #     except:
                #         pass
                    
            except Exception as e:
                print(f"âŒ ToplantÄ± iÅŸleme hatasÄ±: {e}")
                import traceback
                traceback.print_exc()
                
                # Hata durumunda meeting'i hata durumuna al
                db_session = SessionLocal()
                try:
                    error_meeting = db_session.query(Meeting).filter(Meeting.id == meeting.id).first()
                    if error_meeting:
                        error_meeting.status = "error"
                    db_session.commit()
                finally:
                    db_session.close()
            finally:
                loop.close()
    
    background_tasks.add_task(process_meeting)
    db.refresh(meeting)
    
    return meeting


@router.get("/", response_model=List[MeetingResponse])
async def get_meetings(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """KullanÄ±cÄ±nÄ±n toplantÄ±larÄ±nÄ± listele"""
    meetings = db.query(Meeting).filter(
        Meeting.user_id == current_user.id
    ).order_by(Meeting.created_at.desc()).all()
    
    return meetings


@router.get("/{meeting_id}", response_model=MeetingResponse)
async def get_meeting(
    meeting_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ToplantÄ± detayÄ±"""
    meeting = db.query(Meeting).filter(
        Meeting.id == meeting_id,
        Meeting.user_id == current_user.id
    ).first()
    
    if not meeting:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="ToplantÄ± bulunamadÄ±"
        )
    
    return meeting


@router.post("/{meeting_id}/generate-summary")
async def generate_summary(
    meeting_id: int,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ToplantÄ± iÃ§in Ã¶zet oluÅŸtur (manuel)"""
    from ..models import Transcript, Summary
    from ..services.openai_service import OpenAIService
    import asyncio
    
    meeting = db.query(Meeting).filter(
        Meeting.id == meeting_id,
        Meeting.user_id == current_user.id
    ).first()
    
    if not meeting:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="ToplantÄ± bulunamadÄ±"
        )
    
    # ToplantÄ± tamamlanmÄ±ÅŸ mÄ± kontrol et
    if meeting.status != "completed":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Sadece tamamlanmÄ±ÅŸ toplantÄ±lar iÃ§in Ã¶zet oluÅŸturulabilir"
        )
    
    # Zaten Ã¶zet var mÄ± kontrol et
    existing_summary = db.query(Summary).filter(Summary.meeting_id == meeting_id).first()
    if existing_summary:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Bu toplantÄ± iÃ§in zaten Ã¶zet oluÅŸturulmuÅŸ"
        )
    
    # Transkriptleri al
    transcripts = db.query(Transcript).filter(
        Transcript.meeting_id == meeting_id
    ).order_by(Transcript.segment_number).all()
    
    if not transcripts:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="ToplantÄ± iÃ§in transkript bulunamadÄ±"
        )
    
    def create_summary():
        """Background task ile Ã¶zet oluÅŸtur"""
        db_session = SessionLocal()
        try:
            full_text = " ".join([t.text for t in transcripts])
            openai_service = OpenAIService()
            
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                summary_result = loop.run_until_complete(
                    openai_service.summarize_transcript(full_text, meeting.language)
                )
                
                summary = Summary(
                    meeting_id=meeting_id,
                    summary_text=summary_result["summary"],
                    key_points=summary_result["key_points"]
                )
                db_session.add(summary)
                db_session.commit()
            finally:
                loop.close()
        except Exception as e:
            print(f"Ã–zet oluÅŸturma hatasÄ±: {str(e)}")
        finally:
            db_session.close()
    
    background_tasks.add_task(create_summary)
    
    return {"message": "Ã–zet oluÅŸturuluyor, lÃ¼tfen bekleyin..."}


@router.post("/{meeting_id}/cancel")
async def cancel_meeting(
    meeting_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Ä°ÅŸlenmekte olan toplantÄ±yÄ± iptal et"""
    meeting = db.query(Meeting).filter(
        Meeting.id == meeting_id,
        Meeting.user_id == current_user.id
    ).first()
    
    if not meeting:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="ToplantÄ± bulunamadÄ±"
        )
    
    # Sadece iÅŸlenmekte olan toplantÄ±lar iptal edilebilir
    if meeting.status not in ["processing", "recording", "paused"]:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Bu durumda olan toplantÄ±lar iptal edilemez: {meeting.status}"
        )
    
    # ToplantÄ±yÄ± iptal et
    meeting.status = "cancelled"
    meeting.end_time = datetime.utcnow()
    db.commit()
    db.refresh(meeting)
    
    return {
        "message": "ToplantÄ± iptal edildi",
        "meeting_id": meeting.id,
        "status": meeting.status
    }



class ProcessFileRequest(BaseModel):
    audio_file_path: str
    whisper_model: str = "small"  # tiny, base, small, medium, large, speechrecognition, elevenlabs, assemblyai, pyannote
    language: str = "tr"
    use_pyannote: bool = False  # Deprecated: use whisper_model="pyannote" instead
    diarization_profile: str = "auto"
    min_speakers: Optional[int] = None
    max_speakers: Optional[int] = None
    use_streaming: bool = False  # AssemblyAI iÃ§in streaming audio kullan (sadece assemblyai modeli iÃ§in geÃ§erli)


@router.post("/process-file")
async def process_audio_file(
    file_data: ProcessFileRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Dosya yolu ile ses dosyasÄ±nÄ± iÅŸle (Pyannote destekli)"""
    import os
    
    audio_file_path = file_data.audio_file_path
    whisper_model = file_data.whisper_model
    language = file_data.language
    use_pyannote = file_data.use_pyannote
    diarization_profile = file_data.diarization_profile
    min_speakers = file_data.min_speakers
    max_speakers = file_data.max_speakers
    
    # Dosya varlÄ±ÄŸÄ±nÄ± kontrol et
    if not os.path.exists(audio_file_path):
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Ses dosyasÄ± bulunamadÄ±: {audio_file_path}"
        )
    
    # Yeni toplantÄ± oluÅŸtur
    new_meeting = Meeting(
        user_id=current_user.id,
        title=f"Dosya Ä°ÅŸleme - {os.path.basename(audio_file_path)}",
        whisper_model="pyannote" if use_pyannote else whisper_model,
        language=language,
        status="processing",
        start_time=datetime.utcnow(),
        end_time=datetime.utcnow(),
        audio_file_path=audio_file_path,
        use_pyannote=str(use_pyannote).lower() if use_pyannote else None,
        diarization_profile=diarization_profile,
        min_speakers=min_speakers,
        max_speakers=max_speakers
    )
    
    db.add(new_meeting)
    db.commit()
    db.refresh(new_meeting)
    
    # Background task ile iÅŸle
    def process_file():
        import asyncio
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        db_session = SessionLocal()
        
        try:
            # Model tipine gÃ¶re iÅŸleme yap
            model_type = whisper_model if not use_pyannote else "pyannote"
            print(f"ğŸ¯ Dosya iÅŸleme baÅŸlatÄ±lÄ±yor - Model: {model_type}, Dosya: {audio_file_path}")
            
            # Pyannote modeli
            if use_pyannote or model_type == "pyannote":
                print(f"ğŸ¤ Pyannote ile dosya iÅŸleniyor: {audio_file_path}")
                from ..services.pyannote_diarization_service import PyannoteDiarizationService
                pyannote_service = PyannoteDiarizationService()
                
                transcripts = pyannote_service.process_with_speakers(
                    audio_file_path,
                    min_speakers=min_speakers,
                    max_speakers=max_speakers,
                    profile=diarization_profile
                )
                
            # ElevenLabs modeli
            elif model_type == "elevenlabs":
                print(f"ğŸ™ï¸ ElevenLabs ile dosya iÅŸleniyor: {audio_file_path}")
                from ..services.elevenlabs_service import ElevenLabsService
                elevenlabs_service = ElevenLabsService()
                
                transcripts = loop.run_until_complete(
                    elevenlabs_service.transcribe_audio(
                        audio_file_path,
                        model_name="elevenlabs",
                        language=language,
                        enable_speaker_diarization=True
                    )
                )
                
            # AssemblyAI modeli
            elif model_type == "assemblyai":
                print(f"ğŸŒ AssemblyAI ile dosya iÅŸleniyor: {audio_file_path}")
                from ..services.assemblyai_service import AssemblyAIService
                assemblyai_service = AssemblyAIService()
                
                transcripts = loop.run_until_complete(
                    assemblyai_service.transcribe_audio(
                        audio_file_path,
                        model_name="assemblyai",
                        language=language,
                        enable_speaker_diarization=True
                    )
                )
                
            # SpeechRecognition modeli
            elif model_type == "speechrecognition":
                print(f"ğŸ—£ï¸ SpeechRecognition ile dosya iÅŸleniyor: {audio_file_path}")
                from ..services.speechrecognition_service import SpeechRecognitionService
                sr_service = SpeechRecognitionService()
                
                transcripts = loop.run_until_complete(
                    sr_service.transcribe_audio(
                        audio_file_path,
                        model_name="google",
                        language=language,
                        enable_speaker_diarization=False
                    )
                )
                
            # Whisper modelleri (tiny, base, small, medium, large)
            else:
                print(f"ğŸ§ Whisper ({model_type}) ile dosya iÅŸleniyor: {audio_file_path}")
                from ..services.whisper_service import WhisperService
                whisper_service = WhisperService()
                
                transcripts = loop.run_until_complete(
                    whisper_service.transcribe_audio(
                        audio_file_path,
                        model_type,
                        language
                    )
                )
            
            # VeritabanÄ±na kaydet
            from ..models import Transcript
            print(f"ğŸ’¾ {len(transcripts)} transkript segmenti veritabanÄ±na kaydediliyor...")
            
            for i, segment in enumerate(transcripts):
                transcript = Transcript(
                    meeting_id=new_meeting.id,
                    segment_number=i + 1,
                    text=segment["text"],
                    start_time=segment["start"],
                    end_time=segment["end"],
                    speaker_id=segment.get("speaker_id"),
                    speaker_label=segment.get("speaker_label")
                )
                db_session.add(transcript)
            
            updated_meeting = db_session.query(Meeting).filter(Meeting.id == new_meeting.id).first()
            if updated_meeting:
                updated_meeting.status = "completed"
            db_session.commit()
            print(f"âœ… Dosya iÅŸlendi ve kaydedildi - Model: {model_type}")
                
        except Exception as e:
            print(f"âŒ Dosya iÅŸleme hatasÄ±: {e}")
            import traceback
            traceback.print_exc()
            updated_meeting = db_session.query(Meeting).filter(Meeting.id == new_meeting.id).first()
            if updated_meeting:
                updated_meeting.status = "error"
            db_session.commit()
        finally:
            db_session.close()
            loop.close()
    
    background_tasks.add_task(process_file)
    
    return {
        "message": "Dosya iÅŸleniyor...",
        "meeting_id": new_meeting.id
    }


class StreamAudioRequest(BaseModel):
    meeting_id: Optional[int] = None  # Mevcut bir meeting'e baÄŸla (opsiyonel)
    duration_seconds: Optional[int] = None  # Maksimum sÃ¼re (saniye), None ise manuel durdurma
    save_wav: bool = True  # WAV dosyasÄ± kaydet
    language: str = "tr"


@router.post("/stream-audio")
async def stream_audio(
    stream_data: StreamAudioRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    AssemblyAI Streaming Audio - GerÃ§ek zamanlÄ± ses transkripsiyon
    
    NOT: Bu endpoint streaming baÅŸlatÄ±r ve hemen dÃ¶ner.
    Transkriptler gerÃ§ek zamanlÄ± olarak veritabanÄ±na kaydedilir.
    """
    import os
    
    # Meeting kontrolÃ¼ veya yeni meeting oluÅŸtur
    if stream_data.meeting_id:
        meeting = db.query(Meeting).filter(
            Meeting.id == stream_data.meeting_id,
            Meeting.user_id == current_user.id
        ).first()
        
        if not meeting:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ToplantÄ± bulunamadÄ±"
            )
        
        if meeting.status not in ["recording", "paused"]:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Sadece kayÄ±t durumundaki toplantÄ±lara streaming eklenebilir"
            )
    else:
        # Yeni meeting oluÅŸtur
        meeting = Meeting(
            user_id=current_user.id,
            title=f"Streaming Audio - {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            whisper_model="assemblyai",
            language=stream_data.language,
            status="recording",
            start_time=datetime.utcnow()
        )
        db.add(meeting)
        db.commit()
        db.refresh(meeting)
    
    # Background task ile streaming baÅŸlat
    def start_streaming():
        from ..services.assemblyai_streaming_service import AssemblyAIStreamingService
        from ..models import Transcript
        import asyncio
        
        db_session = SessionLocal()
        transcripts_buffer = []
        segment_counter = 0
        
        try:
            print(f"ğŸ™ï¸ AssemblyAI Streaming baÅŸlatÄ±lÄ±yor - Meeting ID: {meeting.id}")
            service = AssemblyAIStreamingService()
            
            # Transcript callback'i
            def on_transcript(text: str, is_formatted: bool):
                nonlocal segment_counter
                if is_formatted and text.strip():
                    segment_counter += 1
                    print(f"ğŸ“ Transkript alÄ±ndÄ± (Segment {segment_counter}): {text[:100]}...")
                    
                    # VeritabanÄ±na kaydet
                    try:
                        transcript = Transcript(
                            meeting_id=meeting.id,
                            segment_number=segment_counter,
                            text=text,
                            start_time=0.0,  # Streaming'de zaman damgasÄ± yok
                            end_time=0.0,
                            speaker_id=None,  # Streaming API'de speaker ID farklÄ± formatta gelebilir
                            speaker_label=None
                        )
                        db_session.add(transcript)
                        db_session.commit()
                        print(f"âœ… Transkript kaydedildi (Segment {segment_counter})")
                    except Exception as e:
                        print(f"âŒ Transkript kaydetme hatasÄ±: {e}")
                        db_session.rollback()
            
            # Session callback'leri
            def on_session_begin(session_id: str, expires_at: int):
                print(f"ğŸŸ¢ Session baÅŸladÄ±: {session_id}")
            
            def on_session_end(audio_duration: float, session_duration: float):
                print(f"ğŸ”´ Session bitti: {audio_duration}s audio, {session_duration}s toplam")
                
                # Meeting'i tamamla
                try:
                    updated_meeting = db_session.query(Meeting).filter(Meeting.id == meeting.id).first()
                    if updated_meeting:
                        updated_meeting.status = "completed"
                        updated_meeting.end_time = datetime.utcnow()
                    db_session.commit()
                    print(f"âœ… Meeting tamamlandÄ± (ID: {meeting.id})")
                except Exception as e:
                    print(f"âŒ Meeting gÃ¼ncelleme hatasÄ±: {e}")
                    db_session.rollback()
            
            # Callback'leri ayarla
            service.set_transcript_callback(on_transcript)
            service.set_session_callbacks(on_session_begin, on_session_end)
            
            # Streaming'i baÅŸlat
            service.start_streaming(duration_seconds=stream_data.duration_seconds)
            
            # WAV dosyasÄ±nÄ± kaydet
            if stream_data.save_wav:
                # Meeting dizinini oluÅŸtur
                meeting_dir = os.path.join("uploads", str(current_user.id), str(meeting.id))
                os.makedirs(meeting_dir, exist_ok=True)
                
                wav_path = os.path.join(meeting_dir, "streaming_audio.wav")
                saved_path = service.save_wav_file(wav_path)
                
                if saved_path:
                    # Meeting'e audio path'i ekle
                    try:
                        updated_meeting = db_session.query(Meeting).filter(Meeting.id == meeting.id).first()
                        if updated_meeting:
                            updated_meeting.audio_file_path = saved_path
                        db_session.commit()
                        print(f"âœ… WAV dosyasÄ± kaydedildi: {saved_path}")
                    except Exception as e:
                        print(f"âŒ Audio path gÃ¼ncelleme hatasÄ±: {e}")
                        db_session.rollback()
            
            print(f"âœ… Streaming tamamlandÄ± - Meeting ID: {meeting.id}")
            
        except Exception as e:
            print(f"âŒ Streaming hatasÄ±: {e}")
            import traceback
            traceback.print_exc()
            
            # Hata durumunda meeting'i hata durumuna al
            try:
                error_meeting = db_session.query(Meeting).filter(Meeting.id == meeting.id).first()
                if error_meeting:
                    error_meeting.status = "error"
                db_session.commit()
            except:
                pass
        finally:
            db_session.close()
    
    # Background task'i baÅŸlat
    background_tasks.add_task(start_streaming)
    
    return {
        "message": "Streaming baÅŸlatÄ±lÄ±yor...",
        "meeting_id": meeting.id,
        "info": "Mikrofona konuÅŸun. Transkriptler gerÃ§ek zamanlÄ± olarak kaydedilecek."
    }


